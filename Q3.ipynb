{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6d9e13",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500390bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_file = open('goemotions.json')\n",
    "data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39147c",
   "metadata": {},
   "source": [
    "### Generate the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Json array to Numpy Array\n",
    "npData = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db440eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece006e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize all the sentences in the dataset\n",
    "sentences_tokenized = []\n",
    "total_n_tokens = 0\n",
    "for sentence in npData[:,0]:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    sentences_tokenized.append(tokens)\n",
    "    total_n_tokens+=len(tokens)\n",
    "print(total_n_tokens)\n",
    "print(len(sentences_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if any sentences got lost\n",
    "print(len(sentences_tokenized) == len(npData[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the tokens the entry way\n",
    "tokenized_npData = []\n",
    "for index in range(0,len(npData[:,0])):\n",
    "    entry = [sentences_tokenized[index], npData[index,1], npData[index,2]]\n",
    "    tokenized_npData.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set it as numpy array, choosing the dtype to be an object\n",
    "tokenized_npData = np.array(tokenized_npData,dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327754ae",
   "metadata": {},
   "source": [
    "## Using the Word Embedder Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734880f8",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb39ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pretrained model\n",
    "word_embedder = downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable counts the number of tokens of which their embeddings were generated\n",
    "n_of_training_tokens_with_embeddings = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ef9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the embeddings for a post\n",
    "word_embed_list = []\n",
    "n_of_words = 0\n",
    "\n",
    "for word in tokenized_npData[0][0]:\n",
    "    try:\n",
    "        word_embedded = word_embedder[word]\n",
    "        word_embed_list.append(word_embedded)\n",
    "        n_of_words = n_of_words + 1\n",
    "        n_of_training_tokens_with_embeddings += 1\n",
    "    except KeyError:\n",
    "        print(\"Key error found for \", word)\n",
    "            \n",
    "if (len(word_embed_list) > 0):\n",
    "    #Assumed to be of the same size\n",
    "    sentence_embedded = np.zeros(len(word_embed_list[0]))\n",
    "    \n",
    "    #Compute the avg of the values of the tokens inside of the sentence\n",
    "    for index in range(0,len(sentence_embedded)):\n",
    "        for word_embedded in word_embed_list:\n",
    "            sentence_embedded[index] += word_embedded[index]\n",
    "        sentence_embedded[index] /= n_of_words\n",
    "            \n",
    "    entry = [sentence_embedded,tokenized_npData[0][1],tokenized_npData[0][2]]\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09487dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the embeddings for all the posts\n",
    "data_sentences_embedded = []\n",
    "n_of_training_tokens_with_embeddings = 0\n",
    "length_of_array = len(tokenized_npData[:,0])\n",
    "\n",
    "for j in range(0,length_of_array):\n",
    "    #Put words embedded in a list\n",
    "    word_embedded_list = []\n",
    "    n_of_words = 0\n",
    "    for word in tokenized_npData[j,0]:\n",
    "        try:\n",
    "            #Generate the embbeds\n",
    "            word_embedded = word_embedder[word]\n",
    "            word_embedded_list.append(word_embedded)\n",
    "            n_of_words = n_of_words + 1\n",
    "            n_of_training_tokens_with_embeddings += 1\n",
    "        except KeyError:\n",
    "            print(\"Key error found for \", word)\n",
    "            \n",
    "   \n",
    "    if (len(word_embedded_list) > 0):\n",
    "         #Assumed all to be of the same size\n",
    "        sentence_embedded = np.zeros(len(word_embedded_list[0]))\n",
    "    \n",
    "        #Compute the avg of the values of the tokens inside of the sentence\n",
    "        for index in range(0,len(sentence_embedded)):\n",
    "            for word_embedded in word_embedded_list:\n",
    "                sentence_embedded[index] += word_embedded[index]\n",
    "            sentence_embedded[index] /= n_of_words\n",
    "            \n",
    "        entry = [sentence_embedded,tokenized_npData[j,1],tokenized_npData[j,2]]\n",
    "        data_sentences_embedded.append(entry)\n",
    "\n",
    "print(\"Number of tokens embedded: \", n_of_training_tokens_with_embeddings)\n",
    "print(\"Percentage of tokens that are embedded: \", (n_of_training_tokens_with_embeddings/total_n_tokens * 100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc703db",
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata_sentences_embedded = np.array(data_sentences_embedded, dtype=object)\n",
    "print(\"Number of entries with sentences embedded: \", len(npdata_sentences_embedded[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ade483",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of entries with sentences embedded: \", len(npdata_sentences_embedded[:,0])/len(tokenized_npData[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc0262",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries for modelling\n",
    "import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d91002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "labEncoder = preprocessing.LabelEncoder()\n",
    "npDataEmotions = labEncoder.fit_transform(npdata_sentences_embedded[:,1])\n",
    "npDataSentiments = labEncoder.fit_transform(npdata_sentences_embedded[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "train_tokens, test_tokens = train_test_split(npdata_sentences_embedded[:,0], test_size=0.2, train_size=0.8, shuffle=False)\n",
    "train_emotions, test_emotions = train_test_split(npDataEmotions, test_size=0.2, train_size=0.8, shuffle=False)\n",
    "train_sentiments, test_sentiments = train_test_split(npDataSentiments, test_size=0.2, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47323b87",
   "metadata": {},
   "source": [
    "#### Base MLP for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_tokens))\n",
    "print(type(train_emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efefecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for the size of array\n",
    "test_token = train_tokens[0]\n",
    "for i in range(1,len(train_tokens)):\n",
    "    if(len(test_token)!=len(train_tokens[i])):\n",
    "        print(i,\" NOT THE SAME: \", len(train_tokens[i])-len(test_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_tokens = []\n",
    "for instance in train_tokens:\n",
    "    fixed_train_tokens.append(np.array(instance,dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_tokens = np.array(fixed_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc94fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_test_tokens = []\n",
    "for instance in test_tokens:\n",
    "    fixed_test_tokens.append(np.array(instance, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_test_tokens = np.array(fixed_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e261d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the emotion CLF\n",
    "emotions_clf = MLPClassifier()\n",
    "emotions_clf.fit(fixed_train_tokens, train_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"emotions_Base_MLP_model_word2vec.sav\"\n",
    "joblib.dump(emotions_clf,open(filename1,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd32d31",
   "metadata": {},
   "source": [
    "#### Base MLP for Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_clf = MLPClassifier()\n",
    "sentiments_clf.fit(fixed_train_tokens, train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = \"sentiments_Base_MLP_model_word2vec.sav\"\n",
    "joblib.dump(sentiments_clf,open(filename2,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58541b77",
   "metadata": {},
   "source": [
    "#### Top MLP for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp_emotions_param = {\n",
    "    'solver': [\"adam\", \"sgd\"],\n",
    "    'hidden_layer_sizes' : [(10,5),(15,10)],\n",
    "    'activation' : [\"relu\", \"tanh\", \"identity\"]\n",
    "}\n",
    "top_mlp_emotions = GridSearchCV(estimator=MLPClassifier(max_iter=2), param_grid=top_mlp_emotions_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp_emotions.fit(fixed_train_tokens, train_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56414f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename3 = \"emotions_Top_MLP_model_word2vec.sav\"\n",
    "joblib.dump(top_mlp_emotions,open(filename3,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea60d8c",
   "metadata": {},
   "source": [
    "#### Top MLP for Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp_sentiments_param = {\n",
    "    'solver': [\"adam\", \"sgd\"],\n",
    "    'hidden_layer_sizes' : [(10,5),(15,10)],\n",
    "    'activation' : [\"relu\", \"tanh\", \"identity\"]\n",
    "}\n",
    "top_mlp_sentiments = GridSearchCV(estimator=MLPClassifier(max_iter=2), param_grid=top_mlp_sentiments_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf66971",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mlp_sentiments.fit(fixed_train_tokens, train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b33301",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename4 = \"sentiments_Top_MLP_model_word2vec.sav\"\n",
    "joblib.dump(top_mlp_emotions,open(filename4,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6e80a",
   "metadata": {},
   "source": [
    "### Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_base_MLP_results = emotions_clf.predict(fixed_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_emotions,emotions_base_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_emotions,emotions_base_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_base_MLP_results=sentiments_clf.predict(fixed_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_sentiments,sentiments_base_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03649eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_sentiments,sentiments_base_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_top_MLP_results = top_mlp_emotions.predict(fixed_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0375b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_emotions,emotions_top_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf407dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_emotions,emotions_top_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_top_MLP_results = top_mlp_sentiments.predict(fixed_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_sentiments,sentiments_top_MLP_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b31c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_sentiments,sentiments_top_MLP_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0df142",
   "metadata": {},
   "source": [
    "## Using the word embedding model: Glove Wikipedia 2014 Gigaword 5th Ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46154f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embbeder_gigaword = downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the embeddings for all the posts\n",
    "data_sentences_embedded_gigaword = []\n",
    "n_of_training_tokens_with_embeddings = 0\n",
    "length_of_array = len(tokenized_npData[:,0])\n",
    "\n",
    "for j in range(0,length_of_array):\n",
    "    #Put words embedded in a list\n",
    "    word_embedded_list = []\n",
    "    n_of_words = 0\n",
    "    for word in tokenized_npData[j,0]:\n",
    "        try:\n",
    "            #Generate the embbeds\n",
    "            word_embedded = word_embbeder_gigaword[word]\n",
    "            word_embedded_list.append(word_embedded)\n",
    "            n_of_words = n_of_words + 1\n",
    "            n_of_training_tokens_with_embeddings += 1\n",
    "        except KeyError:\n",
    "            print(\"Key error found for \", word)\n",
    "            \n",
    "   \n",
    "    if (len(word_embedded_list) > 0):\n",
    "         #Assumed all to be of the same size\n",
    "        sentence_embedded = np.zeros(len(word_embedded_list[0]))\n",
    "    \n",
    "        #Compute the avg of the values of the tokens inside of the sentence\n",
    "        for index in range(0,len(sentence_embedded)):\n",
    "            for word_embedded in word_embedded_list:\n",
    "                sentence_embedded[index] += word_embedded[index]\n",
    "            sentence_embedded[index] /= n_of_words\n",
    "            \n",
    "        entry = [sentence_embedded,tokenized_npData[j,1],tokenized_npData[j,2]]\n",
    "        data_sentences_embedded_gigaword.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens embedded: \", n_of_training_tokens_with_embeddings)\n",
    "print(\"Percentage of tokens that are embedded: \", (n_of_training_tokens_with_embeddings/total_n_tokens * 100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f52f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata_sentences_embedded_gigaword = np.array(data_sentences_embedded_gigaword, dtype=object)\n",
    "print(\"Number of entries with sentences embedded: \", len(npdata_sentences_embedded_gigaword[:,0]))\n",
    "print(\"Percentage of entries with sentences embedded: \", len(npdata_sentences_embedded_gigaword[:,0])/len(tokenized_npData[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9d123",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefe81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "labEncoder = preprocessing.LabelEncoder()\n",
    "gigaword_npDataEmotions = labEncoder.fit_transform(npdata_sentences_embedded_gigaword[:,1])\n",
    "gigaword_npDataSentiments = labEncoder.fit_transform(npdata_sentences_embedded_gigaword[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "gigaword_train_tokens, gigaword_test_tokens = train_test_split(npdata_sentences_embedded_gigaword[:,0], test_size=0.2, train_size=0.8, shuffle=False)\n",
    "gigaword_train_emotions, gigaword_test_emotions = train_test_split(gigaword_npDataEmotions, test_size=0.2, train_size=0.8, shuffle=False)\n",
    "gigaword_train_sentiments, gigaword_test_sentiments = train_test_split(gigaword_npDataSentiments, test_size=0.2, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f532c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gigaword_train_tokens = []\n",
    "for instance in gigaword_train_tokens:\n",
    "    fixed_gigaword_train_tokens.append(np.array(instance,dtype=float))\n",
    "fixed_gigaword_train_tokens = np.array(fixed_gigaword_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14de4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gigaword_test_tokens = []\n",
    "for instance in gigaword_test_tokens:\n",
    "    fixed_gigaword_test_tokens.append(np.array(instance,dtype=float))\n",
    "fixed_gigaword_test_tokens = np.array(fixed_gigaword_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286f2d2",
   "metadata": {},
   "source": [
    "#### Emotions Base MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e04f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaword_emotions_clf = MLPClassifier()\n",
    "gigaword_emotions_clf.fit(fixed_gigaword_train_tokens,gigaword_train_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad802949",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename5 = \"emotions_Base_MLP_model_gigaword.sav\"\n",
    "joblib.dump(gigaword_emotions_clf,open(filename5,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b452cd",
   "metadata": {},
   "source": [
    "#### Sentiments Base MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaword_sentiments_clf = MLPClassifier()\n",
    "gigaword_sentiments_clf.fit(fixed_gigaword_train_tokens,gigaword_train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef111a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename6 = \"sentiments_Base_MLP_model_gigaword.sav\"\n",
    "joblib.dump(gigaword_sentiments_clf,open(filename6,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db9a6b",
   "metadata": {},
   "source": [
    "### Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451406e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaword_base_mlp_emotions_results = gigaword_emotions_clf.predict(fixed_gigaword_test_tokens)\n",
    "gigaword_base_mlp_sentiments_results = gigaword_sentiments_clf.predict(fixed_gigaword_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(gigaword_test_emotions,gigaword_base_mlp_emotions_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb56912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gigaword_test_emotions,gigaword_base_mlp_emotions_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0973ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(gigaword_test_sentiments,gigaword_base_mlp_sentiments_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73730075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gigaword_test_sentiments,gigaword_base_mlp_sentiments_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ebe2f",
   "metadata": {},
   "source": [
    "## Using the word embedding model Fastest Wikipedia News from October 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff82969",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeacf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedder_fwiki = downloader.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the embeddings for all the posts\n",
    "data_sentences_embedded_fwiki = []\n",
    "n_of_training_tokens_with_embeddings = 0\n",
    "length_of_array = len(tokenized_npData[:,0])\n",
    "\n",
    "for j in range(0,length_of_array):\n",
    "    #Put words embedded in a list\n",
    "    word_embedded_list = []\n",
    "    n_of_words = 0\n",
    "    for word in tokenized_npData[j,0]:\n",
    "        try:\n",
    "            #Generate the embbeds\n",
    "            word_embedded = word_embedder_fwiki[word]\n",
    "            word_embedded_list.append(word_embedded)\n",
    "            n_of_words = n_of_words + 1\n",
    "            n_of_training_tokens_with_embeddings += 1\n",
    "        except KeyError:\n",
    "            print(\"Key error found for \", word)\n",
    "            \n",
    "   \n",
    "    if (len(word_embedded_list) > 0):\n",
    "         #Assumed all to be of the same size\n",
    "        sentence_embedded = np.zeros(len(word_embedded_list[0]))\n",
    "    \n",
    "        #Compute the avg of the values of the tokens inside of the sentence\n",
    "        for index in range(0,len(sentence_embedded)):\n",
    "            for word_embedded in word_embedded_list:\n",
    "                sentence_embedded[index] += word_embedded[index]\n",
    "            sentence_embedded[index] /= n_of_words\n",
    "            \n",
    "        entry = [sentence_embedded,tokenized_npData[j,1],tokenized_npData[j,2]]\n",
    "        data_sentences_embedded_fwiki.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens embedded: \", n_of_training_tokens_with_embeddings)\n",
    "print(\"Percentage of tokens that are embedded: \", (n_of_training_tokens_with_embeddings/total_n_tokens * 100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata_sentences_embedded_fwiki = np.array(data_sentences_embedded_fwiki, dtype=object)\n",
    "print(\"Number of entries with sentences embedded: \", len(npdata_sentences_embedded_fwiki[:,0]))\n",
    "print(\"Percentage of entries with sentences embedded: \", len(npdata_sentences_embedded_fwiki[:,0])/len(tokenized_npData[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f8bef",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "labEncoder = preprocessing.LabelEncoder()\n",
    "fwiki_npDataEmotions = labEncoder.fit_transform(npdata_sentences_embedded_fwiki[:,1])\n",
    "fwiki_npDataSentiments = labEncoder.fit_transform(npdata_sentences_embedded_fwiki[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89742e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "fwiki_train_tokens, fwiki_test_tokens = train_test_split(npdata_sentences_embedded_fwiki[:,0], test_size=0.2, train_size=0.8, shuffle=False)\n",
    "fwiki_train_emotions, fwiki_test_emotions = train_test_split(fwiki_npDataEmotions, test_size=0.2, train_size=0.8, shuffle=False)\n",
    "fwiki_train_sentiments, fwiki_test_sentiments = train_test_split(fwiki_npDataSentiments, test_size=0.2, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fwiki_train_tokens = []\n",
    "for instance in fwiki_train_tokens:\n",
    "    fixed_fwiki_train_tokens.append(np.array(instance,dtype=float))\n",
    "fixed_fwiki_train_tokens = np.array(fixed_fwiki_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fwiki_test_tokens = []\n",
    "for instance in fwiki_test_tokens:\n",
    "    fixed_fwiki_test_tokens.append(np.array(instance,dtype=float))\n",
    "fixed_fwiki_test_tokens = np.array(fixed_fwiki_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe789d6",
   "metadata": {},
   "source": [
    "#### Base MLP Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958dab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwiki_emotions_clf = MLPClassifier()\n",
    "fwiki_emotions_clf.fit(fixed_fwiki_train_tokens,fwiki_train_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cb878",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename7 = \"emotions_Base_MLP_model_fwiki.sav\"\n",
    "joblib.dump(fwiki_emotions_clf,open(filename7,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef8d53",
   "metadata": {},
   "source": [
    "#### Base MLP Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85340c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwiki_sentiments_clf = MLPClassifier()\n",
    "fwiki_sentiments_clf.fit(fixed_fwiki_train_tokens,fwiki_train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename8 = \"sentiments_Base_MLP_model_fwiki.sav\"\n",
    "joblib.dump(fwiki_sentiments_clf,open(filename8,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaa019",
   "metadata": {},
   "source": [
    "### Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwiki_base_mlp_emotions_results = fwiki_emotions_clf.predict(fixed_fwiki_test_tokens)\n",
    "fwiki_base_mlp_sentiments_results = fwiki_sentiments_clf.predict(fixed_fwiki_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ace59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(fwiki_test_emotions,fwiki_base_mlp_emotions_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ef854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(fwiki_test_emotions,fwiki_base_mlp_emotions_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98810e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(fwiki_test_sentiments,fwiki_base_mlp_sentiments_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(fwiki_test_sentiments,fwiki_base_mlp_sentiments_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
